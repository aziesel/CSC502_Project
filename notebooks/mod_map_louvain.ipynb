{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install GraphFrames using the following command with your Virtual Environment activated and pyspark already installed:\n",
    "# pyspark --packages graphframes:graphframes:0.6.0-spark2.3-s_2.11\n",
    "from itertools import islice\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from les_mis import LES_MIS_GRAPH\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "graph = sc.parallelize(LES_MIS_GRAPH, 32)\n",
    "NUM_PARTITIONS = graph.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = graph.flatMap(lambda x: (x[0], x[1])).distinct().collect()\n",
    "node_to_id = {node: i for i, node in enumerate(nodes)}\n",
    "id_to_node = {i: node for i, node in enumerate(nodes)}\n",
    "node_id_to_cluster = {i: f\"cluster:{i}\" for i in range(len(nodes))}\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERS: dict[str, dict[int, tuple[str, int]]] = {v:{k: (\"move\", 0)} for k, v in node_id_to_cluster.items()}\n",
    "list(islice(CLUSTERS.items(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_attributes(graph: pyspark.rdd.RDD, node_to_id: dict[int, str]):\n",
    "    \"\"\"\n",
    "    Function is designed to take all nodes in a graph and output their node ID's and their attributes\n",
    "    to a dictionary that can be queried when we want to easily access the attributes of a node.\n",
    "    \"\"\"\n",
    "    encoded_edges = [(node_to_id[edge[0]], node_to_id[edge[1]], edge[2]) for edge in graph.collect()]\n",
    "    node_attributes = {i:([], []) for i, node in enumerate(nodes)}\n",
    "    for entry in encoded_edges:\n",
    "        node_attributes[entry[0]][0].append(entry[1])\n",
    "        node_attributes[entry[0]][1].append(entry[2])\n",
    "        node_attributes[entry[1]][0].append(entry[0])\n",
    "        node_attributes[entry[1]][1].append(entry[2])\n",
    "\n",
    "    return node_attributes\n",
    "\n",
    "node_id_to_attrs = get_node_attributes(graph, node_to_id)\n",
    "list(islice(node_id_to_attrs.items(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cluster_map(node_id: int, new_cluster_id: str):\n",
    "    # function that updates the cluster id that maps to each respective node id\n",
    "    node_id_to_cluster[node_id] = new_cluster_id\n",
    "\n",
    "def generate_test_clusters(nodes: list[int]):\n",
    "    # generate a test cluster map\n",
    "    # modifies global objects\n",
    "    for index in range(int(len(nodes) / 2)):\n",
    "        node  = nodes[index]\n",
    "        adj_node = node_id_to_attrs[node][0][0] # get first adjacent node\n",
    "        node_id_to_cluster[adj_node] = node_id_to_cluster[node]\n",
    "        # add the adjacent node to the cluster\n",
    "\n",
    "generate_test_clusters(list(id_to_node.keys()))\n",
    "list(islice(node_id_to_cluster.items(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_input_c(graph: pyspark.rdd.RDD):\n",
    "    return graph.flatMap(lambda x: (x[0], x[1])).distinct().zipWithIndex().map(lambda x: (node_to_id[x[0]], f\"cluster:{x[1]}\"))\n",
    "\n",
    "def update_input_c(graph: pyspark.rdd.RDD):\n",
    "    # Update the input vertex to cluster mapping based on the updated cluster values\n",
    "    return graph.map(lambda x: (x[0], node_id_to_cluster[x[0]])).sortByKey()\n",
    "\n",
    "def init_input_g(graph: pyspark.rdd.RDD):\n",
    "    \"\"\"\n",
    "    Get the input graph in the format (v, [list of edges and their degree], [])\n",
    "    \"\"\"\n",
    "    return graph.map(lambda x: (x[0], (x[1], x[2]))).union(\n",
    "        graph.map(lambda x: (x[1], (x[0], x[2])))\n",
    "    ).map(\n",
    "        lambda x: (node_to_id[x[0]], ((node_to_id[x[1][0]], node_id_to_attrs[node_to_id[x[1][0]]]), x[1][1]))\n",
    "    ).groupByKey().mapValues(lambda r: ([x[0] for x in r], [x[1] for x in r])).sortByKey(lambda x: x[0])\n",
    "\n",
    "input_c = init_input_c(graph)\n",
    "input_c.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_c = update_input_c(input_c) # updated based on test update cluster for now\n",
    "input_c.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Input G part 1\n",
    "input_g = init_input_g(graph)\n",
    "input_g.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip doesn't work with the modifications to input c and g respectively\n",
    "# so we needed to repartition at this stage in the algorithm\n",
    "input_c = sc.parallelize(input_c.collect(), NUM_PARTITIONS).sortByKey(lambda x: x[0])\n",
    "input_g = sc.parallelize(input_g.collect(), NUM_PARTITIONS).sortByKey(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_zip = input_c.zip(input_g).map(lambda x: ( x[0][1], ([(x[0][0], x[1][1])]) ) ).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the vertices such that the existing cluster takes a new node...\n",
    "first_agg = first_zip.mapValues(lambda c: [(v[0], e, v[1][1]) for v in c for e in v[1][0]])\n",
    "first_agg.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cluster = [\n",
    "    (0, # \n",
    "        (\n",
    "            [\n",
    "                (46, ([27, 75, 25, 52, 28, 35, 33, 4, 20, 56, 48, 22, 17, 0, 18], [3, 3, 3, 3, 4, 4, 4, 2, 9, 2, 1, 5, 1, 1, 2])),\n",
    "                (18, ([0, 20], [2, 3]))\n",
    "            ],\n",
    "            [1, 2] \n",
    "        )\n",
    "     ),\n",
    "    (46,\n",
    "        (\n",
    "            [  \n",
    "                (46, ([0, 27, 75, 25], [1, 3, 3, 3])),\n",
    "                (18, ([0, 20], [2, 3]))\n",
    "            ],\n",
    "            [1, 2]\n",
    "        ),\n",
    "        [27, 75, 25, 52, 28, 35, 33, 4, 20, 56, 48, 22, 17, 0, 18], [3, 3, 3, 3, 4, 4, 4, 2, 9, 2, 1, 5, 1, 1, 2]\n",
    "    )\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper Definitions from page 690 in Distributed Graph Clustering Using Modularity Map Equation:\n",
    "# deg(v) - The weighted degree of a node v that is the sum of all outgoing edges (v, u) of v.\n",
    "# vol(C) - The sum of all weighted degrees of a set of nodes C.\n",
    "# cut(v, C) - The sum of all weights of edges (v, u) where u is in C.\n",
    "\n",
    "# cluster 0 expected outputs:\n",
    "# \n",
    "# vol(C \\ v) = 0\n",
    "# cut(v, C \\ v) = sum([3, 3, 3, 3, 4, 4, 4, 2, 9]) + sum([1, 2])\n",
    "# cut "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    # Could try using this and serializing the object in each stage for the graph...\n",
    "    def __init__(self, cluster_id: str, neighbors: list[tuple[str, int]]):\n",
    "        self.id = id\n",
    "        self.cluster_id = cluster_id\n",
    "        self.neighbors = [neighbor for neighbor, _ in neighbors]\n",
    "        self.weights = [weight for _, weight in neighbors]\n",
    "    \n",
    "    def update_cluster_id(self, cluster_id: str):\n",
    "        self.cluster_id = cluster_id\n",
    "\n",
    "    def get_volume(self, node_id: int):\n",
    "        for i in range(len(self.neighbors)):\n",
    "            if self.neighbors[i] == node_id:\n",
    "                return self.weights[i]    \n",
    "\n",
    "    def get_degree(self):\n",
    "        return sum(self.weights)\n",
    "\n",
    "    def get_neighbours(self):\n",
    "        return self.neighbors\n",
    "\n",
    "    def get_cut(self, other):\n",
    "        return len(set(self.neighbors).intersection(set(other.neighbors)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc502-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Jul 10 2022, 16:26:13) [GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0147c8f441cbbd5705f6a0ae768b3f83c267d6fc5a57c28fe60fc48d36eb859e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
